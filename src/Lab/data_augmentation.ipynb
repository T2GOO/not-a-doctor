{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions deprecated here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a>**Function definition**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tgoor\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, save_img\n",
    "from PIL import Image, ImageEnhance\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import kagglehub, os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_uniform_noise(image, minval=-1, maxval=1):\n",
    "    noise = tf.random.uniform(shape=tf.shape(image), minval=minval, maxval=maxval)\n",
    "    noisy_image = image + noise\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_and_save_images(input_dir, output_dir, augmentations_per_image=5):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,          # Rotation des images\n",
    "        horizontal_flip=True,       # Symétrie horizontale\n",
    "        fill_mode='nearest',        # Remplir les zones vides après transformation\n",
    "    )\n",
    "    if os.path.isdir(input_dir):\n",
    "        for img_name in os.listdir(input_dir):\n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            \n",
    "            # Charger l'image\n",
    "            img = load_img(img_path)\n",
    "            img_array = img_to_array(img)\n",
    "            # add noise\n",
    "            img_array = add_uniform_noise(img_array)\n",
    "\n",
    "            img_array = tf.reshape(img_array, (1,) + img_array.shape)  # Ajouter une dimension batch\n",
    "\n",
    "            # Générer des augmentations et les sauvegarder\n",
    "            count = 0\n",
    "            for batch in datagen.flow(img_array, batch_size=1, save_to_dir=output_dir, save_prefix=\"aug\", save_format=\"jpg\"):\n",
    "                count += 1\n",
    "                if count >= augmentations_per_image:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_salt_and_pepper_noise(image, salt_prob=0.001, pepper_prob=0.001):\n",
    "    image_np = image.numpy()\n",
    "    num_salt = int(salt_prob * image_np.size)\n",
    "    num_pepper = int(pepper_prob * image_np.size)\n",
    "\n",
    "    # Add salt (white pixels)\n",
    "    coords_salt = [np.random.randint(0, i - 1, num_salt) for i in image_np.shape]\n",
    "    image_np[coords_salt[0], coords_salt[1], :] = 1\n",
    "\n",
    "    # Add pepper (black pixels)\n",
    "    coords_pepper = [np.random.randint(0, i - 1, num_pepper) for i in image_np.shape]\n",
    "    image_np[coords_pepper[0], coords_pepper[1], :] = 0\n",
    "    return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_data (data_dir:str) -> int:\n",
    "    state = 0\n",
    "    folder = data_dir\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "            state = 1\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_dataset (dst:str = '\\\\data\\\\current', src:str = '\\\\data\\\\archive') -> int:\n",
    "    root = os.getcwd()\n",
    "    dst = root + dst\n",
    "    src = root + src\n",
    "    if not os.path.exists(src) or not os.path.exists(dst):\n",
    "        print (f'Error: {src} or {dst} not found')\n",
    "        return 1\n",
    "    try:\n",
    "        # clear data from current data dir\n",
    "        clear_data (dst)\n",
    "        print(\"Dataset updating...\")\n",
    "        # merge train and test folder into train\n",
    "        os.mkdir(os.path.join(dst, 'train'))\n",
    "        os.mkdir(os.path.join(dst, 'train', '0'))\n",
    "        os.mkdir(os.path.join(dst, 'train', '1'))\n",
    "        for suffix in ('train', 'test'):\n",
    "            for class_ in ('0', '1'):\n",
    "                path_src = os.path.join(src, suffix, class_)\n",
    "                path_dst = os.path.join(dst, 'train', class_)\n",
    "                for item in os.listdir(path_src):\n",
    "                    source_item = os.path.join(path_src, item)\n",
    "                    destination_item = os.path.join(path_dst, item)\n",
    "                    # Copy each item (handle files and directories)\n",
    "                    if os.path.isdir(source_item):\n",
    "                        shutil.copytree(source_item, destination_item, dirs_exist_ok=True)\n",
    "                    else:\n",
    "                        shutil.copy2(source_item, destination_item)\n",
    "        # copy valid\n",
    "        path_src = os.path.join(src, 'valid')\n",
    "        path_dst = os.path.join(dst, 'valid')\n",
    "        os.mkdir(path_dst)\n",
    "        for item in os.listdir(path_src):\n",
    "            source_item = os.path.join(path_src, item)\n",
    "            destination_item = os.path.join(path_dst, item)\n",
    "            # Copy each item (handle files and directories)\n",
    "            if os.path.isdir(source_item):\n",
    "                shutil.copytree(source_item, destination_item, dirs_exist_ok=True)\n",
    "            else:\n",
    "                shutil.copy2(source_item, destination_item)\n",
    "\n",
    "        print(\"Dataset updated\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return 1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importlib_dataset_from_kagglehub(dataset_url:str = \"hayder17/breast-cancer-detection\", force:bool = True) -> int:\n",
    "    dst = os.path.join(os.getcwd(), 'data', 'current')\n",
    "    arch = os.path.join(os.getcwd(), 'data', 'archive')\n",
    "    if not os.path.exists(arch) or not os.path.exists(dst):\n",
    "        print (f'Error: {arch} or {dst} not found')\n",
    "        return 1\n",
    "        \n",
    "    if not dataset_url:\n",
    "        print(\"Error: Path to dataset is empty.\")\n",
    "        return 1\n",
    "\n",
    "    try:\n",
    "        clear_data (arch)\n",
    "        # Extract dataset name from the path\n",
    "        cache_path = kagglehub.dataset_download(dataset_url, force_download=force)\n",
    "        # move from .cache to dst\n",
    "        for item in os.listdir(cache_path):\n",
    "            source_item = os.path.join(cache_path, item)\n",
    "            destination_item = os.path.join(arch, item)\n",
    "\n",
    "            # Copy each item (handle files and directories)\n",
    "            if os.path.isdir(source_item):\n",
    "                shutil.copytree(source_item, destination_item, dirs_exist_ok=True)\n",
    "            else:\n",
    "                shutil.copy2(source_item, destination_item)\n",
    "\n",
    "        # copy dataset from archive to current\n",
    "        reset_dataset()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return 1\n",
    "    print(\"Dataset imported\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a>**-- SCRIPT FROM HERE --**</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset dataset (data/current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt.reset_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt.augment_and_save_images(os.path.join(os.getcwd(), 'data', 'archive', 'train', 'Covid'), os.path.join(os.getcwd(), 'data', 'current', 'train', 'Covid'), 2)\n",
    "# dt.augment_and_save_images(os.path.join(os.getcwd(), 'data', 'archive', 'train', 'Normal'), os.path.join(os.getcwd(), 'data', 'current', 'train', 'Normal'), 2)\n",
    "# dt.augment_and_save_images(os.path.join(os.getcwd(), 'data', 'archive', 'train', 'Viral Pneumonia'), os.path.join(os.getcwd(), 'data', 'current', 'train', 'Viral Pneumonia'), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt.augment_and_save_images(os.path.join(os.getcwd(), 'data', 'archive', 'test', 'Covid'), os.path.join(os.getcwd(), 'data', 'current', 'test', 'Covid'), 2)\n",
    "# dt.augment_and_save_images(os.path.join(os.getcwd(), 'data', 'archive', 'test', 'Normal'), os.path.join(os.getcwd(), 'data', 'current', 'test', 'Normal'), 2)\n",
    "# dt.augment_and_save_images(os.path.join(os.getcwd(), 'data', 'archive', 'test', 'Viral Pneumonia'), os.path.join(os.getcwd(), 'data', 'current', 'test', 'Viral Pneumonia'), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
